package detector

import (
	"context"
	"fmt"
	"math"
	"strconv"
	"time"

	"github.com/giantswarm/microerror"
	"github.com/giantswarm/micrologger"
	corev1 "k8s.io/api/core/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

const (
	defaultMaxNodeTerminationPercentage = 0.10
	defaultNotReadyTickThreshold        = 6
	defaultPauseBetweenTermination      = time.Minute * 10

	nodeNotReadyDuration = time.Second * 30

	annotationNodeNotReadyTick = "giantswarm.io/node-not-ready-tick"
	labelNodeRole              = "role"
	labelNodeRoleMaster        = "master"
	labelNodeRoleWorker        = "worker"
)

var trueConditions = []string{
	fmt.Sprintf("%s", corev1.NodeReady),
}
var falseConditions = []string{
	// Custom condition generated by https://github.com/giantswarm/node-problem-detector-app
	"DiskFull",
}

type Config struct {
	Logger    micrologger.Logger
	K8sClient client.Client

	// MaxNodeTerminationPercentage defines a maximum percentage of nodes that will be returned as 'marked for termination'
	// ie: if the value is 0.5 and cluster have 10 nodes, than `DetectBadNodes`can only return maximum of 5 nodes
	// marked for termination at single run
	MaxNodeTerminationPercentage float64
	// NotReadyTickThreshold defines a how many times the node must bee seen as NotReady in order to return it as 'marked for termination'
	NotReadyTickThreshold int
	// PauseBetweenTermination defines a pause between 2 intervals where node termination can occur.
	// This is a safeguard to prevent nodes being terminated over and over or to not terminate too much at once.
	// ie: if the value is 5m it means once it returned nodes for termination it wont return another nodes for another 5 min.
	PauseBetweenTermination time.Duration
}

type Detector struct {
	logger    micrologger.Logger
	k8sClient client.Client

	maxNodeTerminationPercentage float64
	notReadyTickThreshold        int
	pauseBetweenTermination      time.Duration
}

func NewDetector(config Config) (*Detector, error) {
	if config.Logger == nil {
		return nil, microerror.Maskf(invalidConfigError, "%T.Logger must not be empty", config)
	}
	if config.K8sClient == nil {
		return nil, microerror.Maskf(invalidConfigError, "%T.K8sClient must not be empty", config)
	}

	if config.MaxNodeTerminationPercentage == 0 {
		config.MaxNodeTerminationPercentage = defaultMaxNodeTerminationPercentage
	}
	if config.NotReadyTickThreshold == 0 {
		config.NotReadyTickThreshold = defaultNotReadyTickThreshold
	}
	if config.PauseBetweenTermination == 0 {
		config.PauseBetweenTermination = defaultPauseBetweenTermination
	}

	d := &Detector{
		logger:    config.Logger,
		k8sClient: config.K8sClient,

		maxNodeTerminationPercentage: config.MaxNodeTerminationPercentage,
		notReadyTickThreshold:        config.NotReadyTickThreshold,
		pauseBetweenTermination:      config.PauseBetweenTermination,
	}

	return d, nil
}

// DetectBadNodes will return list of nodes that should be terminated which in documentation terminology is used as 'marked for termination'.
func (d *Detector) DetectBadNodes(ctx context.Context) ([]corev1.Node, error) {
	var nodeList corev1.NodeList
	{
		err := d.k8sClient.List(ctx, &nodeList)
		if err != nil {
			return nil, microerror.Mask(err)
		}
	}

	// badNodes list will contain all nodes that reached tick threshold and are 'marked for termination'
	var badNodes []corev1.Node
	for i, n := range nodeList.Items {
		notReadyTickCount, updated := nodeNotReadyTickCount(ctx, d.logger, n)

		if notReadyTickCount >= d.notReadyTickThreshold {
			badNodes = append(badNodes, n)
		}

		// if the tick counter changed, we need to update the value in the k8s api
		if updated {
			// update the tick count on the node
			n.Annotations[annotationNodeNotReadyTick] = fmt.Sprintf("%d", notReadyTickCount)
			err := d.k8sClient.Update(ctx, &nodeList.Items[i])
			if err != nil {
				return nil, microerror.Mask(err)
			}
			d.logger.LogCtx(ctx, "level", "debug", "message", fmt.Sprintf("updated not ready tick count to %d/%d for node %s", notReadyTickCount, d.notReadyTickThreshold, n.Name))
		}
	}

	// remove additional master nodes to avoid multiple master node termination at the same time
	badNodes = removeMultipleMasterNodes(badNodes)
	d.logger.LogCtx(ctx, "level", "debug", "message", fmt.Sprintf("found %d nodes marked for termination", len(badNodes)))

	// check for node termination limit, to prevent termination of all nodes at once
	maxNodeTermination := maximumNodeTermination(len(nodeList.Items), d.maxNodeTerminationPercentage)
	if len(badNodes) > maxNodeTermination {
		badNodes = badNodes[:maxNodeTermination]
		d.logger.LogCtx(ctx, "level", "debug", "message", fmt.Sprintf("limited node termination to %d nodes", maxNodeTermination))
	}

	return badNodes, nil
}

// ResetTickCounters will reset tick counters to zero on all k8s nodes in a cluster
func (d *Detector) ResetTickCounters(ctx context.Context) error {
	var nodeList corev1.NodeList
	{
		err := d.k8sClient.List(ctx, &nodeList)
		if err != nil {
			return microerror.Mask(err)
		}
	}

	for i, node := range nodeList.Items {
		if _, ok := node.GetAnnotations()[annotationNodeNotReadyTick]; ok {
			node.Annotations[annotationNodeNotReadyTick] = "0"

			err := d.k8sClient.Update(ctx, &nodeList.Items[i])
			if err != nil {
				return microerror.Mask(err)
			}
		}
	}

	return nil
}

// isNodeUnhealthy returns true of the node is not ready for certain period of time
// this is used to detect bad nodes
func isNodeUnhealthy(ctx context.Context, logger micrologger.Logger, n corev1.Node) bool {
	// trueConditions have to be true, otherwise node has to be considered unhealthy.
	for _, trueCondition := range trueConditions {
		for _, c := range n.Status.Conditions {
			if fmt.Sprintf("%s", c.Type) == trueCondition && c.Status != corev1.ConditionTrue {
				// We want condition to be true, but it's not.
				if time.Since(c.LastHeartbeatTime.Time) >= nodeNotReadyDuration {
					logger.Debugf(ctx, "node %s is unhealthy because we expected condition %s to be true, but was false", n.Name, c.Type)
					return true
				}
			}
		}
	}

	// falseConditions have to be false, otherwise node has to be considered unhealthy.
	for _, falseCondition := range falseConditions {
		for _, c := range n.Status.Conditions {
			if fmt.Sprintf("%s", c.Type) == falseCondition && c.Status == corev1.ConditionTrue {
				// we want condition to be false, but it's not.
				if time.Since(c.LastHeartbeatTime.Time) >= nodeNotReadyDuration {
					logger.Debugf(ctx, "node %s is unhealthy because we expected condition %s to be false, but was true", n.Name, c.Type)
					return true
				}
			}
		}
	}
	return false
}

// updateNodeNotReadyTickAnnotations will update annotations on the node
// depending if the node is Ready or not
// the annotation is used to track how many times node was seen as not ready
// and in case it will reach a threshold, the node will be marked for termination.
// Each run of this function can increase or decrease the tick count by 1.
// function return a tick counter (int) and a bool indicating if the value changed
func nodeNotReadyTickCount(ctx context.Context, logger micrologger.Logger, n corev1.Node) (int, bool) {
	var err error
	updated := false

	// fetch current notReady tick count from node
	// if there is no annotation yet, the value will be 0
	notReadyTickCount := 0
	{
		tick, ok := n.Annotations[annotationNodeNotReadyTick]
		if ok {
			notReadyTickCount, err = strconv.Atoi(tick)
			// in case the annotation is a garbage lets reset to 0 and update it
			if err != nil {
				notReadyTickCount = 0
				updated = true
			}
		}
	}

	// increase or decrease the tick count depending on the node status
	if isNodeUnhealthy(ctx, logger, n) {
		notReadyTickCount++
		updated = true
	} else if notReadyTickCount > 0 {
		notReadyTickCount--
		updated = true
	}

	return notReadyTickCount, updated
}

// maximumNodeTermination calculates the maximum number of nodes that can be terminated on single run
// the number is calculated with help of maxNodeTerminationPercentage
// which determines how much percentage of nodes can be terminated
// the minimum is 1 node termination per run
func maximumNodeTermination(nodeCount int, maxNodeTerminationPercentage float64) int {
	limit := math.Round(float64(nodeCount) * maxNodeTerminationPercentage)

	if limit < 1 {
		limit = 1
	}
	return int(limit)
}

// removeMultipleMasterNodes removes multiple master nodes from the list to avoid more than 1 master node termination at same time
// worker nodes in the list are unaffected
func removeMultipleMasterNodes(nodeList []corev1.Node) []corev1.Node {
	foundMasterNode := false
	// filteredNodes list will contain maximum 1 master node and unlimited number of worker nodes at the end of the function
	var filteredNodes []corev1.Node

	for _, n := range nodeList {
		if n.Labels[labelNodeRole] == labelNodeRoleMaster {
			// append only the first master that is found in the list
			// any following master is not appended to the final list
			if !foundMasterNode {
				filteredNodes = append(filteredNodes, n)
				foundMasterNode = true
			} else {
				// removing additional master nodes from the list
				continue
			}
		} else {
			// append all non-master nodes
			filteredNodes = append(filteredNodes, n)
		}
	}
	return filteredNodes
}
